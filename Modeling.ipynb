{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8IJS, homo sapience nanobody sequence\n",
    "nb_seq = \"MDVQLVESGGGLVNPGGSLRLSCAASGRTFSSYSMGWFRQAPGKEREFVVAISKGGYKYDAVSLEGRFTISRDNAKNTVYLQINSLRPEDTAVYYCASSRAYGSSRLRLADTYEYWGQGTLVTVSS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constants for sequence preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_types = \"ACDEFGHIKLMNPQRSTVWY\" # residue types\n",
    "res_to_n = {x: i for i, x in enumerate(res_types)} # create {res : idx} dictionary\n",
    "# atom_types = [\"N\", \"CA\", \"C\", \"O\", \"CB\", \"CG\", \"CD\", \"NE\", \"NZ\", \"OH\"]\n",
    "res_to_num = lambda x: res_to_n[x] if x in res_to_n else len(res_to_n) # res string to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(targets, nb_classes=21):\n",
    "    print(targets)\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape) + [nb_classes])\n",
    "\n",
    "def get_encoding(sequence):\n",
    "    one_hot_amino = get_one_hot(np.array([res_to_num(x) for x in sequence]))\n",
    "    return one_hot_amino\n",
    "\n",
    "# def encoding(sequence):\n",
    "#     nb_classes = len(res_types)\n",
    "#     res_to_num = np.array([res_to_num(x) for x in sequence])\n",
    "\n",
    "#     res = np.eye(nb_classes)[np.array(res_to_num).reshape(-1)]\n",
    "#     encoding = res.reshape(list(targets.shape) + [nb_classes])\n",
    "#     return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  2 17 13  9 17  3 15  5  5  5  9 17 11 12  5  5 15  9 14  9 15  1  0\n",
      "  0 15  5 14 16  4 15 15 19 15 10  5 18  4 14 13  0 12  5  8  3 14  3  4\n",
      " 17 17  0  7 15  8  5  5 19  8 19  2  0 17 15  9  3  5 14  4 16  7 15 14\n",
      "  2 11  0  8 11 16 17 19  9 13  7 11 15  9 14 12  3  2 16  0 17 19 19  1\n",
      "  0 15 15 14  0 19  5 15 15 14  9 14  9  0  2 16 19  3 19 18  5 13  5 16\n",
      "  9 17 16 17 15 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = get_encoding(nb_seq)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 126\n",
    "origin = torch.zeros(seq_len, 3) # rigid origin, 3dim\n",
    "rot = torch.eye(3).unsqueeze(0).expand(seq_len,-1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \"MDVQL\"\n",
    "dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rel_pos(sequence, rel_pos_dim): # node의 상대 위치 반환\n",
    "    # broadcasting에 의해 tensor 크기 맞춰짐 (seq_len, seq_len)\n",
    "    rel_pos = torch.arange(len(sequence)).unsqueeze(-1) - torch.arange(len(sequence)).unsqueeze(0)\n",
    "\n",
    "    # rel_pos_dim 의 두 배 만큼 clamp, 모든 값 양수 되도록 조정\n",
    "    rel_pos = rel_pos.clamp(min=-rel_pos_dim, max=rel_pos_dim) # + rel_pos_dim\n",
    "    return rel_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = get_encoding(seq)\n",
    "sequence_dict = {\"H\", seq}\n",
    "\n",
    "encoding = torch.tensor(get_encoding(sequence_dict), dtype=torch.get_default_dtype())\n",
    "\n",
    "def node_rel_pos(node_features, rel_pos_dim):\n",
    "    relative_positions = (torch.arange(node_features.shape[-2])[None] - torch.arange(node_features.shape[-2])[:, None])\n",
    "    relative_positions = relative_positions.clamp(min=-rel_pos_dim, max=rel_pos_dim) + rel_pos_dim\n",
    "    return relative_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvariantPointAttention(torch.nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, heads=12, head_dim=16, n_query_points=4, n_value_points=8, **kwargs):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.head_dim = head_dim\n",
    "        self.n_query_points = n_query_points\n",
    "\n",
    "        node_scalar_attention_inner_dim = heads * head_dim\n",
    "        node_vector_attention_inner_dim = 3 * n_query_points * heads\n",
    "        node_vector_attention_value_dim = 3 * n_value_points * heads\n",
    "        after_final_cat_dim = heads * edge_dim + heads * head_dim + heads * n_value_points * 4\n",
    "\n",
    "        point_weight_init_value = torch.log(torch.exp(torch.full((heads,), 1.)) - 1.)\n",
    "        self.point_weight = torch.nn.Parameter(point_weight_init_value)\n",
    "\n",
    "        self.to_scalar_qkv = torch.nn.Linear(node_dim, 3 * node_scalar_attention_inner_dim, bias=False)\n",
    "        self.to_vector_qk = torch.nn.Linear(node_dim, 2 * node_vector_attention_inner_dim, bias=False)\n",
    "        self.to_vector_v = torch.nn.Linear(node_dim, node_vector_attention_value_dim, bias=False)\n",
    "        self.to_scalar_edge_attention_bias = torch.nn.Linear(edge_dim, heads, bias=False)\n",
    "        self.final_linear = torch.nn.Linear(after_final_cat_dim, node_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.final_linear.weight.fill_(0.0)\n",
    "            self.final_linear.bias.fill_(0.0)\n",
    "\n",
    "    def forward(self, node_features, edge_features, rigid):\n",
    "        # Classic attention on nodes\n",
    "        scalar_qkv = self.to_scalar_qkv(node_features).chunk(3, dim=-1)\n",
    "        scalar_q, scalar_k, scalar_v = map(lambda t: rearrange(t, 'n (h d) -> h n d', h=self.heads), scalar_qkv)\n",
    "        node_scalar = torch.einsum('h i d, h j d -> h i j', scalar_q, scalar_k) * self.head_dim ** (-1 / 2)\n",
    "\n",
    "        # Linear bias on edges\n",
    "        edge_bias = rearrange(self.to_scalar_edge_attention_bias(edge_features), 'i j h -> h i j')\n",
    "\n",
    "        # Reference frame attention\n",
    "        wc = (2 / self.n_query_points) ** (1 / 2) / 6\n",
    "        vector_qk = self.to_vector_qk(node_features).chunk(2, dim=-1)\n",
    "        vector_q, vector_k = map(lambda x: vec_from_tensor(rearrange(x, 'n (h p d) -> h n p d', h=self.heads, d=3)),\n",
    "                                 vector_qk)\n",
    "        rigid_ = rigid.unsqueeze(0).unsqueeze(-1)  # add head and point dimension to rigids\n",
    "\n",
    "        global_vector_k = rigid_ @ vector_k\n",
    "        global_vector_q = rigid_ @ vector_q\n",
    "        global_frame_distance = wc * global_vector_q.unsqueeze(-2).dist(global_vector_k.unsqueeze(-3)).sum(\n",
    "            -1) * rearrange(self.point_weight, \"h -> h () ()\")\n",
    "\n",
    "        # Combining attentions\n",
    "        attention_matrix = (3 ** (-1 / 2) * (node_scalar + edge_bias - global_frame_distance)).softmax(-1)\n",
    "\n",
    "        # Obtaining outputs\n",
    "        edge_output = (rearrange(attention_matrix, 'h i j -> i h () j') * rearrange(edge_features,\n",
    "                                                                                    'i j d -> i () d j')).sum(-1)\n",
    "        scalar_node_output = torch.einsum('h i j, h j d -> i h d', attention_matrix, scalar_v)\n",
    "\n",
    "        vector_v = vec_from_tensor(\n",
    "            rearrange(self.to_vector_v(node_features), 'n (h p d) -> h n p d', h=self.heads, d=3))\n",
    "        global_vector_v = rigid_ @ vector_v\n",
    "        attended_global_vector_v = global_vector_v.map(\n",
    "            lambda x: torch.einsum('h i j, h j p -> h i p', attention_matrix, x))\n",
    "        vector_node_output = rigid_.inv() @ attended_global_vector_v\n",
    "        vector_node_output = torch.stack(\n",
    "            [vector_node_output.norm(), vector_node_output.x, vector_node_output.y, vector_node_output.z], dim=-1)\n",
    "\n",
    "        # Concatenate along heads and points\n",
    "        edge_output = rearrange(edge_output, 'n h d -> n (h d)')\n",
    "        scalar_node_output = rearrange(scalar_node_output, 'n h d -> n (h d)')\n",
    "        vector_node_output = rearrange(vector_node_output, 'h n p d -> n (h p d)')\n",
    "\n",
    "        combined = torch.cat([edge_output, scalar_node_output, vector_node_output], dim=-1)\n",
    "\n",
    "        return node_features + self.final_linear(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
